{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(train_path=\"\", dev_path=\"\", test_path=\"\", as_df=True):\n",
    "    \"\"\"transform pandas dataset into transformer datasets.dataset\n",
    "\n",
    "    Args:\n",
    "        sliding_window (str, optional): Token limit avoiding approaches. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        datasets.dataset: dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data = pd.read_csv(train_path)\n",
    "    dev_data = pd.read_csv(dev_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    train_data[\"idx\"] = [i for i in range(train_data[\"question\"].size)]\n",
    "    dev_data[\"idx\"] = [i for i in range(dev_data[\"question\"].size)]\n",
    "    test_data[\"idx\"] = [i for i in range(test_data[\"question\"].size)]\n",
    "\n",
    "    if(as_df):\n",
    "        return train_data, dev_data\n",
    "    else:\n",
    "        dataset_train = Dataset.from_pandas(train_data)\n",
    "        dataset_dev = Dataset.from_pandas(dev_data)\n",
    "        dataset_test = Dataset.from_pandas(test_data)\n",
    "\n",
    "        complete_ds = DatasetDict(\n",
    "            {\"train\": dataset_train, \"dev\": dataset_dev, \"test\": dataset_test}\n",
    "        )\n",
    "        \n",
    "        return complete_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"input\"], example[\"output\"], truncation=True, max_length=512\n",
    "    )  \n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    print([(l, p) for l, p in list(zip(labels, preds)) if l == 1][:10])\n",
    "    # calculate accuracy using sklearn's function\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(\"dev set len:\", len(tokenized_datasets[\"dev\"]), \"pred len:\", len(preds))\n",
    "    print(\"F1:\", f1, \"Acc:\", acc)\n",
    "    return {\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "\n",
    "def combine_splitted_samples(dataset):\n",
    "    \"\"\"split the dataset in many datasets where all samples are combined with the same idx_complete\n",
    "\n",
    "    Args:\n",
    "        dataset (huggingface datasets): dataset to be splitted\n",
    "\n",
    "    Returns:\n",
    "        list: list of hugginface datasets\n",
    "    \"\"\"\n",
    "    ds_list = []\n",
    "    temp_dict = {}\n",
    "    for sample in dataset:\n",
    "        if sample[\"idx_complete\"] not in temp_dict:\n",
    "            temp_dict[sample[\"idx_complete\"]] = dict(\n",
    "                map(lambda x: (x, []), sample.keys())\n",
    "            )\n",
    "        for key in temp_dict[sample[\"idx_complete\"]].keys():\n",
    "            temp_dict[sample[\"idx_complete\"]][key].append(sample[key])\n",
    "\n",
    "    for sample in temp_dict.values():\n",
    "        ds_list.append(Dataset.from_dict(sample))\n",
    "\n",
    "    return ds_list\n",
    "\n",
    "\n",
    "def predict_on_dataset(list_dataset):\n",
    "    \"\"\"Predict all samples in a list of datasets and take the mean of the predictions as the final prediction\n",
    "\n",
    "    Args:\n",
    "        list_dataset (list): list of small datasets containing all sub samples of a complete sample\n",
    "\n",
    "    Returns:\n",
    "        tuple: prediction and correct label as two lists\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    gt = []\n",
    "    for sub_dataset in list_dataset:\n",
    "        sub_predicition = trainer.predict(sub_dataset)\n",
    "        # calc the average of the predictions\n",
    "        sub_predicition = sub_predicition.predictions.mean(axis=0)\n",
    "        preds.append(sub_predicition.argmax())\n",
    "        # gt.append(sub_dataset[0][\"label\"])\n",
    "    return preds, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_ds = create_dataset(sliding_window=\"keep_question\", train_path=\"data/train.csv\", dev_path=\"data/dev.csv\", test_path=\"data/final_test.csv\", as_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'nlpaueb/legal-bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "tokenized_datasets = legal_ds.map(tokenize_function, batched=True)\n",
    "samples = tokenized_datasets[\"train\"][\"input_ids\"][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(checkpoint + \"_finetuneFFF2\"),\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=4,  \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=100,\n",
    "    bf16=True,\n",
    "    learning_rate=2.2493504069942526e-05,\n",
    "    warmup_steps=10,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_strategy=\"epoch\",\n",
    "    seed=7854,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    "    metric_for_best_model=\"f1_score\",\n",
    "    weight_decay = 0.015631376305494087,\n",
    "    # disable_tqdm=True\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=-0.05)]\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Eval\n",
    "# Combine all samples with the same idx_complete into a single batch for testing\n",
    "dev_dataset_list = combine_splitted_samples(tokenized_datasets[\"dev\"])\n",
    "# predict over dev dataset list and combine for each list entry all predictions to a final prediction\n",
    "preds, gt = predict_on_dataset(dev_dataset_list)\n",
    "\n",
    "# Score dev set\n",
    "print(\"F1 Score (Macro)\", f1_score(preds, gt, average=\"macro\"))\n",
    "print(\"F1 Score (binary)\", f1_score(preds, gt, average=\"micro\"))\n",
    "print(\"Accuracy\", accuracy_score(preds, gt))\n",
    "results.append(\n",
    "    (\n",
    "        f1_score(preds, gt, average=\"macro\"),\n",
    "        f1_score(preds, gt, average=\"micro\"),\n",
    "        accuracy_score(preds, gt),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataset_list = combine_splitted_samples(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, gt = predict_on_dataset(pred_dataset_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
